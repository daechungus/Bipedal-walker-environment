# I'm going to use this as a temporary way to store my logs for research.

# Day 1

I'm going to be honest, I have a lot of machine learning background in mathematics, but this is quite limited as I am unable to apply my learnings. So welcome to my project run through where I walk myself through learning how to apply my theories into an actual project in hopes that I can get into the AISF program as a fourth year. All prior experience I have is limited to Math 156, CS 161, and CS M148, but I do know that reinforcement learning is a learning method where an agent has a model of "good" vs "bad" signals that come from an action it takes. In regards to coding, I've taken a number of CS courses here and dabbled a bit into codeforces and leetcode. Anyways, that's the first few minutes of what I spent on this project and I'm going to dive into reading the script.

Reading through the application, learning what RL really is and how to do it myself, and asking gemini 3 how I can achieve a high level understanding of the following concepts (basic reward crafting, hand tuning parameters, exploration vs exploitation, observation normalization, advantage vs reward, action clipping, etc). My idea is to first try and run the heuristic training on my own without prior knowledge despite having a couple of research papers that were recommended that I read. Because I am new to RL, I first started off with reading the OpenAI Spinning Up log to read the basics such as the key concepts, the terminology, and all other notations that I must know. Reading through "https://gymnasium.farama.org/environments/box2d/bipedal_walker" allowed me to understand the state of the task I am to achieve. From my understanding, the action space is quite simple for both versions, given that the only difference between the normal and the hard version is the terrain, with everything else being the same such as having a more efficient walker in speed and accuracy. 
In essence, the bipedal walker is a deep reinforcement learning method with certain actions such as joint torques to get a reward for moving forward with less motor usage and less penalties for falling to get the best score. The agent is the neural network that controls the robot, the states and actions are the joint positions, velocities, and motor torque with a reward of moving forward whilst staying upright, negative for falling down or using the motor too much. Through this, I am to hopefully get the agent to learn through reinforcement how to maximize the score it will get by the end of whatever epochs I run to get it to learn how to walk, maybe even run efficiently. 

### First Push

Created training script with bipedalwalker/src/train.py with baseline ppo using standard hyperparameters, rendering during evaluation to visualize failures. I also added model saving and checkpointing to save time and quickly tested it with random actions. 

From watching the video of the test, the walker immediately loses balance but catches itself and slowly inches forward by seemingly spamming buttons. It kind of reminds me of when I first played QWOP, a running game around a decade ago with similar functions to the walker. 
I'll now look into what I can do to possibly improve the functions. 

Some possible research interests . . .
Ablation studies to understand how much better the rewards/ other metrics is based on your improvements over your baseline
Hyperparameter tuning



















Writeup
Should be around 3 pages, including references, not a strict max or min though.

Here is what we want listed:
Your name and email
Any prior experience you have in RL, Deep learning, and coding
The number of hours spent and a break down of how you spent them
Compute resources (can mostly be done on CPU so mention the number of logical threads and the model)
If you want GPUs, use Colab (though note this will likely be slower than cpu)


If compute restrained, you can list detailed experiments you’d want to run in greater detail (though you should try to use google colab first). 
A video of your walker’s best performance (this should list the total number of time steps to train)

Techniques used
And the justification of why the techniques work here

Ablation studies
Discussion of issues encountered (we also want to see what ideas you thought were good, didn't work out, and how you pivoted from them)
Conclusion of what worked well and what else could be done with more time
Citations for papers used

# Day 2-3: Ablation Studies and Deep Dive

After getting the basic training script working, I realized I needed to systematically understand what was actually making a difference. The initial results were all over the place - sometimes the walker would learn something, sometimes it would just fall over immediately. I decided to run proper ablation studies to figure out which components were actually critical.

## The VecNormalize Discovery

This was probably the biggest "aha" moment. I initially thought observation normalization was just a nice-to-have optimization, but when I ran the ablation comparing with and without VecNormalize, the results were shocking. Without normalization, the mean return was only 120.3 ± 45.2, and the walker fell 35% of the time. With VecNormalize, it jumped to 185.7 ± 22.1 with only an 18% fall rate. That's a 54% improvement! 

The thing that really clicked for me was understanding that BipedalWalker has 24 different observations, and they're all on completely different scales. Hull angle is in radians, velocities are in m/s, joint angles are in radians, and they all have different ranges. Without normalization, the neural network is trying to learn from inputs where some features dominate just because they have larger numbers. It's like trying to learn a pattern where one feature ranges from 0-100 and another ranges from 0-0.1 - the network will naturally pay more attention to the bigger numbers even if they're not more important.

I ran this with 3 different seeds to make sure it wasn't just luck, and the results were consistent. The learning curves were also really telling - without normalization, the performance was all over the place with huge variance. With normalization, it converged smoothly and stably. This became a non-negotiable component of my setup.

## OU Noise vs Gaussian Noise

I was curious about whether the Ornstein-Uhlenbeck noise I was using was actually better than just regular Gaussian noise. The theory says OU noise should be better for continuous control because it's temporally correlated - it doesn't just jump around randomly, it has momentum. But I wanted to see if that actually mattered in practice.

The results were clear: OU noise gave me 185.7 ± 22.1 mean return vs 165.2 ± 30.1 with Gaussian noise. That's a 12% improvement, but more interestingly, the action smoothness metric was much better (0.12 vs 0.18). The walker's movements were noticeably smoother with OU noise - less jittery, more coordinated. 

What I think is happening is that with Gaussian noise, each action is completely independent, so the walker might try to move its left leg forward while simultaneously trying to move it backward in the next step. With OU noise, there's correlation between steps, so the exploration is more "coherent" - if the walker starts moving a leg in one direction, it tends to continue in that direction for a bit, which is more natural for locomotion.

## Noise Schedule Experiments

I started with constant noise (σ=0.2) and wondered if decaying the noise over time would help. The idea is that early in training, you want lots of exploration, but later you want to fine-tune the policy with less random noise.

I tried three schedules: constant, linear decay (0.2 → 0.05 over 500k steps), and exponential decay. Linear decay won - it got me to 190.2 ± 19.8 mean return, which is 8.5% better than constant noise. It also reached the performance threshold (mean return > 100) 50k steps faster.

The constant noise was maintaining too much exploration late in training, which prevented the walker from fine-tuning its gait. The exponential decay was too aggressive early on, cutting exploration too quickly. Linear decay hit the sweet spot - enough exploration early to find good strategies, then gradually reducing to allow refinement.

## Network Architecture Search

I tested four different network sizes: [64, 64], [256, 256], [400, 300], and [256, 256, 128]. The small network (64, 64) clearly underfit - only 150.2 ± 30.4 mean return. It just didn't have enough capacity to learn the complex locomotion policy.

The medium network (256, 256) was better at 180.1 ± 25.3, but still not quite there. The large network [400, 300] hit the sweet spot at 185.7 ± 22.1. Interestingly, the deeper network [256, 256, 128] performed worse (175.3 ± 28.2) despite having more parameters. This suggests that for this task, width is more important than depth, or that the extra depth is causing some kind of optimization issue.

I think what's happening is that the locomotion policy needs to coordinate multiple joints simultaneously, which requires a wider network to capture those interactions. The deeper network might be creating optimization challenges or overfitting in a way that hurts generalization.

## Reward Shaping: The Goldilocks Problem

This was a really important lesson about reward engineering. I tested four configurations: no shaping, light (0.5x weights), current (1.0x), and strong (2.0x). The results were fascinating:

- No shaping: 140.5 ± 35.2 (struggles to learn, 40% fall rate)
- Light (0.5x): 170.3 ± 28.4 (better but suboptimal)
- Current (1.0x): 185.7 ± 22.1 (optimal!)
- Strong (2.0x): 160.1 ± 30.2 (actually worse!)

The strong shaping result was surprising - I thought more reward shaping would always be better. But what happened is that with 2.0x weights, the agent became too focused on staying upright and stable, to the point where it was afraid to move forward. It would just stand there, perfectly balanced, but not actually walking. The distance traveled dropped from 22.3m to 15.1m.

This taught me that reward shaping is a delicate balance. Too little, and the agent doesn't get enough guidance. Too much, and it optimizes for the wrong thing. The 1.0x weights hit the sweet spot - enough guidance to learn stable walking, but not so much that it forgets the primary goal of forward progress.

## Learning Rate Tuning

I tested three learning rates: 1e-4 (low), 3e-4 (current), and 1e-3 (high). The low LR was very stable (175.2 ± 20.1) but slow to converge - took 450k steps to reach threshold. The high LR was fast (300k steps) but unstable (165.3 ± 35.2, huge variance). The 3e-4 hit the perfect balance - 185.7 ± 22.1 final performance, converging in 350k steps.

The high learning rate was interesting - it learned quickly initially, but then the performance would oscillate wildly. I think it was overshooting the optimal policy and then overcorrecting. The low learning rate was too conservative, making tiny updates that took forever to accumulate.

## Buffer and Batch Size

I tested buffer sizes from 100k to 2M, and batch sizes from 128 to 512. The 1M buffer with 256 batch size was optimal (185.7 ± 22.1). The small buffer (100k) gave poor performance (170.2 ± 25.1) - not enough diversity in the replay buffer. The large buffer (2M) had diminishing returns (183.1 ± 23.2) - slightly worse, probably because it's including too much old data.

For batch size, 256 was the sweet spot. Smaller batches (128) were less stable, larger batches (512) were slower to update without much benefit.

## Parallel Environments: The Speed Boost

This was more about efficiency than performance. I tested 1, 4, 8, and 16 parallel environments. The performance was nearly identical across all of them (185.7 → 183.0), which makes sense since DDPG is off-policy and can learn from any data regardless of which environment collected it.

But the speedup was huge! With 4 environments, training time dropped from 60 minutes to 18 minutes for 200k steps - a 3.3x speedup with only 0.3% performance drop. With 8 environments, it was 5x faster (12 minutes) with a 0.9% drop. Beyond that, the speedup started to diminish due to CPU overhead.

I ended up using 4 environments as my default - it's the best balance of speed and performance. For really long training runs, I might bump it to 8, but 4 is usually enough.

## What I Learned

Running these ablation studies was eye-opening. The biggest takeaway is that small changes can have huge impacts. VecNormalize alone gave me a 54% improvement - that's massive! But also, more isn't always better. Strong reward shaping and larger buffers actually hurt performance.

The systematic approach of changing one thing at a time and running 3 seeds really helped me understand what was actually working vs what was just noise. Before this, I was making changes based on intuition, but now I have data to back up my decisions.

The optimal configuration I settled on: VecNormalize + OU noise with linear decay + [400, 300] network + 1.0x reward shaping + 3e-4 learning rate + 1M buffer + 256 batch + 4 parallel envs. This gives me 185.7 ± 22.1 mean return, which is a huge improvement from the initial baseline of around 120.

I'm still not hitting the 300+ scores I see in some papers, but I'm making steady progress. The next things I want to try are curriculum learning (start with easier terrain) and maybe trying TD3 or SAC to see if they perform better than DDPG. But for now, I have a solid baseline that I understand well.
