{
  "experiments": [
    {
      "experiment_id": "A",
      "experiment_name": "DDPG Baseline",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "max_epsilon": 0.2,
        "min_epsilon": 0.1,
        "decay_steps": 10000,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 500000 timesteps",
      "start_time": "2025-12-27T22:09:50.102087",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "A",
      "experiment_name": "DDPG Baseline",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "max_epsilon": 0.2,
        "min_epsilon": 0.1,
        "decay_steps": 10000,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 5000 timesteps",
      "start_time": "2025-12-27T22:12:31.581114",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "A",
      "experiment_name": "DDPG Baseline",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "max_epsilon": 0.2,
        "min_epsilon": 0.1,
        "decay_steps": 10000,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T22:13:06.962408",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "A",
      "experiment_name": "DDPG Baseline",
      "baseline": "PPO + Reward Wrapper",
      "variables": {
        "algorithm": "PPO",
        "use_reward_wrapper": true,
        "n_envs": 8,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T22:15:13.451972",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "A",
      "experiment_name": "DDPG + Ornstein-Uhlenbeck",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 500000 timesteps",
      "start_time": "2025-12-27T22:56:32.547636",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "A",
      "experiment_name": "DDPG + OU Noise rerun",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T23:06:26.319669",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "10",
      "experiment_name": "DDPG + OU Noise rerun 2",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T23:11:48.011707",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "11",
      "experiment_name": "DDPG + OU Noise rerun 3",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T23:13:16.570849",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "12",
      "experiment_name": "DDPG + OU Noise with new rewards",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 10000 timesteps",
      "start_time": "2025-12-27T23:22:20.526234",
      "status": "completed",
      "metrics": {
        "mean_reward": -95.245717,
        "std_reward": 0.0,
        "max_reward": -95.245717,
        "min_reward": -95.245717,
        "mean_distance": -9.527344600000001,
        "max_distance": -9.527344600000001,
        "mean_episode_length": -95.273446,
        "stability_score": 0.470227048125,
        "total_timesteps": 10000,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": [],
      "end_time": "2025-12-27T23:22:53.457713"
    },
    {
      "experiment_id": "13",
      "experiment_name": "DDPG + OU Noise with new rewards",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1
      },
      "description": "Training for 10000 timesteps",
      "start_time": "2025-12-27T23:23:43.439842",
      "status": "completed",
      "metrics": {
        "mean_reward": -95.783515,
        "std_reward": 0.0,
        "max_reward": -95.783515,
        "min_reward": -95.783515,
        "mean_distance": -9.5856856,
        "max_distance": -9.5856856,
        "mean_episode_length": -95.856856,
        "stability_score": 0.4700447325,
        "total_timesteps": 10000,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": [],
      "end_time": "2025-12-27T23:24:18.609515"
    },
    {
      "experiment_id": "14",
      "experiment_name": "DDPG + OU Noise with joint velocity rewards",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 10000 timesteps",
      "start_time": "2025-12-27T23:26:56.067031",
      "status": "completed",
      "metrics": {
        "mean_reward": -113.223263,
        "std_reward": 0.0,
        "max_reward": -113.223263,
        "min_reward": -113.223263,
        "mean_distance": -11.310027100000001,
        "max_distance": -11.310027100000001,
        "mean_episode_length": -113.100271,
        "stability_score": 0.4646561653125,
        "total_timesteps": 10000,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": [],
      "end_time": "2025-12-27T23:30:10.329067"
    },
    {
      "experiment_id": "14",
      "experiment_name": "DDPG + OU Noise with joint velocity rewards",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 10000 timesteps",
      "start_time": "2025-12-27T23:28:22.011919",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "15",
      "experiment_name": "DDPG + OU Noise with new rewards",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 10000 timesteps",
      "start_time": "2025-12-27T23:31:40.294727",
      "status": "completed",
      "metrics": {
        "mean_reward": -102.213825,
        "std_reward": 0.0,
        "max_reward": -102.213825,
        "min_reward": -102.213825,
        "mean_distance": -10.0668236,
        "max_distance": -10.0668236,
        "mean_episode_length": -100.668236,
        "stability_score": 0.46854117625,
        "total_timesteps": 10000,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": [],
      "end_time": "2025-12-27T23:32:14.476592"
    },
    {
      "experiment_id": "EXP_014",
      "experiment_name": "RewardWrapper + DDPG + GaussianNoise",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 10000000 timesteps",
      "start_time": "2025-12-27T23:35:50.223721",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "16",
      "experiment_name": "DDPG + OU Noise with new rewards",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T23:41:16.097022",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "17",
      "experiment_name": "DDPG + OU Noise with new rewards",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T23:41:34.917659",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "18",
      "experiment_name": "DDPG + OU Noise with new penalties",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T23:42:30.719548",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "19",
      "experiment_name": "DDPG + OU Noise with new penalties",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 50000 timesteps",
      "start_time": "2025-12-27T23:45:20.812843",
      "status": "completed",
      "metrics": {
        "mean_reward": 38.818652,
        "std_reward": 0.0,
        "max_reward": 38.818652,
        "min_reward": 38.818652,
        "mean_distance": 3.7733106999999997,
        "max_distance": 3.7733106999999997,
        "mean_episode_length": 37.733107,
        "stability_score": 0.5117915959375,
        "total_timesteps": 100000,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": [],
      "end_time": "2025-12-28T18:16:48.835092"
    },
    {
      "experiment_id": "19",
      "experiment_name": "DDPG + OU Noise run 2",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 100000 timesteps",
      "start_time": "2025-12-28T17:28:05.698647",
      "status": "running",
      "metrics": {
        "mean_reward": null,
        "std_reward": null,
        "max_reward": null,
        "min_reward": null,
        "mean_distance": null,
        "max_distance": null,
        "mean_episode_length": null,
        "stability_score": null,
        "total_timesteps": null,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": []
    },
    {
      "experiment_id": "20",
      "experiment_name": "DDPG + OU Noise",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 1,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 100000 timesteps",
      "start_time": "2025-12-28T19:53:12.843821",
      "status": "completed",
      "metrics": {
        "mean_reward": -143.175185,
        "std_reward": 0.0,
        "max_reward": -143.175185,
        "min_reward": -143.175185,
        "mean_distance": -15.1353012,
        "max_distance": -15.1353012,
        "mean_episode_length": -151.353012,
        "stability_score": 0.45270218375,
        "total_timesteps": 100000,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": [],
      "end_time": "2025-12-28T20:38:38.778728"
    },
    {
      "experiment_id": "21",
      "experiment_name": "DDPG + OU Noise",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 8,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 200000 timesteps",
      "start_time": "2025-12-28T20:51:04.571845",
      "status": "completed",
      "metrics": {
        "mean_reward": 80.459597,
        "std_reward": 0.0,
        "max_reward": 80.459597,
        "min_reward": 80.459597,
        "mean_distance": 3.4416368,
        "max_distance": 3.4416368,
        "mean_episode_length": 34.416368,
        "stability_score": 0.510755115,
        "total_timesteps": 200000,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": [],
      "end_time": "2025-12-28T21:04:06.633423"
    },
    {
      "experiment_id": "22",
      "experiment_name": "DDPG + OU Noise",
      "baseline": "DDPG + Reward Wrapper",
      "variables": {
        "algorithm": "DDPG",
        "use_reward_wrapper": true,
        "n_envs": 8,
        "ou_noise_theta": 0.15,
        "ou_noise_sigma": 0.2,
        "stay_upright_bonus": 0.1,
        "symmetry_penalty_weight": 0.1,
        "joint_velocity_bonus_weight": 0.02
      },
      "description": "Training for 400000 timesteps",
      "start_time": "2025-12-28T21:15:29.168618",
      "status": "completed",
      "metrics": {
        "mean_reward": -93.670458,
        "std_reward": 0.0,
        "max_reward": -93.670458,
        "min_reward": -93.670458,
        "mean_distance": -9.8270364,
        "max_distance": -9.8270364,
        "mean_episode_length": -98.270364,
        "stability_score": 0.46929051125,
        "total_timesteps": 400000,
        "training_time_seconds": null
      },
      "failures": [],
      "pivots": [],
      "notes": [],
      "end_time": "2025-12-28T21:51:52.430200"
    }
  ],
  "last_updated": "2025-12-28T21:51:52.431705"
}